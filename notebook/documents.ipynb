{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8819f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Document Data-Structure\n",
    "\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c26a8f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'xyz', 'pages': 1, 'author': 'kushagra', 'date_created': '2026-01-01'}, page_content='this is the main text to create a RAG')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc= Document(\n",
    "    page_content= \"this is the main text to create a RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"xyz\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"kushagra\",\n",
    "        \"date_created\":\"2026-01-01\"\n",
    "\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2906e521",
   "metadata": {},
   "source": [
    "## MANUAL LOADER\n",
    "\n",
    "flattened structured fields into a single dense line with | separators.\n",
    "\n",
    "That kills semantic boundaries. Embeddings don’t understand “fields”, they understand language structure.\n",
    "\n",
    "\"Index:1|Name:Thermostat Drone Heater|Description:Consumer approach...\" is embedding garbage\n",
    "\n",
    "# Use a manual loader only if:\n",
    "\n",
    "the official loader cannot extract text correctly at all, or\n",
    "\n",
    "you need non-standard parsing (weird formats, mixed encodings, broken structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244f9af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,\n",
       " Document(metadata={'source': 'sample.csv', 'row': 0}, page_content='Index:1|Name:Thermostat Drone Heater|Description:Consumer approach woman us those star.|Brand:Bradford-Yu|Category:Kitchen Appliances|Price:74|Currency:USD|Stock:139|EAN:8619793560985|Color:Orchid|Size:Medium|Availability:backorder|Internal ID:38'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/sample.csv\")\n",
    "documents=[]\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = \"|\".join(f\"{col}:{row[col]}\" for col in df.columns )\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content= text,\n",
    "        metadata={\n",
    "            \"source\": \"sample.csv\",\n",
    "            \"row\":idx\n",
    "        }\n",
    "    )\n",
    "\n",
    "    documents.append(doc)\n",
    "len(documents), documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7303e1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'sample.csv', 'row': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8bec3f",
   "metadata": {},
   "source": [
    "# LANGCHAIN LOADER\n",
    "\n",
    "Each field is clearly separated.\n",
    "\n",
    "Newlines preserve semantic breaks.\n",
    "\n",
    "Metadata is clean and minimal.\n",
    "\n",
    "This will chunk better, retrieve better, and hallucinate less.\n",
    "\n",
    "\n",
    "that said it is not that manual loader is bad it can come to good when used with a better script and then you can manipulate the metadata tags to keep much more relevant info going forward.\n",
    "\n",
    "# extra\n",
    "\n",
    "If it’s useful for meaning, it belongs in page_content.\n",
    "\n",
    "If it’s useful for control or tracing, it belongs in metadata.\n",
    "\n",
    "\n",
    "A better loader does two independent things well:\n",
    "\n",
    "1.produces clean, human-readable page-content.\n",
    "\n",
    "2.produces minimal, accurate metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "513531a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Index: 1\n",
      "Name: Thermostat Drone Heater\n",
      "Description: Consumer approach woman us those star.\n",
      "Brand: Bradford-Yu\n",
      "Category: Kitchen Appliances\n",
      "Price: 74\n",
      "Currency: USD\n",
      "Stock: 139\n",
      "EAN: 8619793560985\n",
      "Color: Orchid\n",
      "Size: Medium\n",
      "Availability: backorder\n",
      "Internal ID: 38\n",
      "{'source': '../data/sample.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path=\"../data/sample.csv\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c3be2",
   "metadata": {},
   "source": [
    "# now working with some pdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ff5cb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Mumma's Kitchen\n",
      "Twister\n",
      "Sandwich  &\n",
      "Hot Dog\n",
      "Cheese Chilli \n",
      "Peri Peri \n",
      "Schezwan \n",
      "Lemon Chilli \n",
      "Italian \n",
      "Tandoori Masala\n",
      "Cheese Chutney\n",
      "Cheese Chilli \n",
      "Chatpata Indori\n",
      "Cheese Corn \n",
      "Paneer Schezwan\n",
      "Cheese Burst\n",
      "Chatpata Hot Dog \n",
      "Veg Aalu Tikki Hot Dog \n",
      "Paneer Tikka Hot Dog \n",
      "₹ 85\n",
      "₹ 95\n",
      "₹ 115\n",
      "₹ 105\n",
      "₹ 125\n",
      "₹ 135\n",
      "₹ 129\n",
      "₹ 135\n",
      "₹ 129\n",
      "₹ 89\n",
      "₹ 99\n",
      "₹ 99\n",
      "₹ 99\n",
      "₹ 89\n",
      "₹ 89\n",
      "Pizzas\n",
      "Pasta\n",
      "Go-To \n",
      "Indie-Mexican \n",
      "Oh-Cheese! \n",
      "Desi Chirpira \n",
      "Toofani Mexican\n",
      "Crunchy Kurkure \n",
      "Peri-Peri Spicy\n",
      "Pro-Max Cheese \n",
      "Paneer Shaukeen\n",
      "Cheesy Fries Supreme\n",
      "Sab Par Bhari \n",
      "Pasta Arrabiata \n",
      "(Penne pasta tossed in authentic\n",
      "red sauce)\n",
      "Pasta Alfredo \n",
      "(Penne pasta tossed in creamy\n",
      "white sauce)\n",
      "Baked Cheesy Pasta\n",
      "(Arrabiata pasta baked to\n",
      "perfection with extra cheese)\n",
      "Baked Alfredo \n",
      "Green Wave \n",
      "(Capsicum, Jalapeno, Onion) \n",
      "Farm Fresh \n",
      "(The evergreen combination of Onion and Capsicum) \n",
      "Margherita \n",
      "(The classic pizza sauce and mozzarella cheese) \n",
      "Corn Feast \n",
      "(Golden Corn and lots of cheese) \n",
      "Veggie Blast \n",
      "(Capsicum, Onion, Golden Corn, Olive)\n",
      "Smoky Hot \n",
      "(A spice lover's heaven with Red Paprika, Jalapeno and Onion) \n",
      "Mushroom Riot \n",
      "(Smoky Grilled Mushrooms with Onion, Capsicum and Olives) \n",
      "3 Sins of Cheese \n",
      "(Three types of cheese with cheese burst crust) \n",
      "Paneer Patola \n",
      "(Indian Twist of Tandoori Paneer, Onion, Capsicum, Red\n",
      "Paprika) \n",
      "Grandiose \n",
      "(Onion, Capsicum, Olives, red paprika, Golden Corn, Jalapeno)\n",
      "₹ 125\n",
      "₹ 125\n",
      "₹ 135\n",
      "₹ 140\n",
      "₹ 199\n",
      "₹ 199\n",
      "₹ 189\n",
      "₹ 210\n",
      "₹ 235\n",
      "₹ 230\n",
      "₹ 79\n",
      "₹ 89\n",
      "₹ 99\n",
      "₹ 105\n",
      "₹ 125\n",
      "₹ 99\n",
      "₹ 105\n",
      "₹ 135\n",
      "₹ 129\n",
      "₹ 139\n",
      "₹ 149\n",
      "₹ 169\n",
      "₹ 195\n",
      "₹ 215\n",
      "₹235\n",
      "Patties/Puff\n",
      "Garlic Breads\n",
      "Masala Patties\n",
      "Cheese patties\n",
      "Cheese-Corn\n",
      "patties\n",
      "Pizza Patties \n",
      "Paneer Barbeque\n",
      "Patties\n",
      "Cheese Corn \n",
      "Paprika & Olives\n",
      "Jalapeno & Onion\n",
      "Cheesy Bread Sticks\n",
      "Stuffed Cheese Corn\n",
      "Stuffed tandoori\n",
      "Paneer\n",
      "₹ 120\n",
      "₹ 135\n",
      "₹ 135\n",
      "₹ 145\n",
      "₹ 185\n",
      "₹ 199\n",
      "₹ 35\n",
      "₹ 55\n",
      " ₹ 70\n",
      "₹ 65\n",
      "₹ 75\n",
      "Burgers\n",
      "Sides\n",
      "Shakes &\n",
      "Coffee\n",
      "French Fries - Salted\n",
      "Peri Peri French Fries\n",
      "Cheesy French Fries\n",
      "Cheesy Peri Peri Fries\n",
      "Super Loaded Fries\n",
      "Kulhad Maggi \n",
      "Korean Cheese Bun \n",
      "Veg Fried Momos\n",
      "Paneer Fried Momos\n",
      "Fried Cheesy Momos\n",
      "Mexican Nacho Bowl\n",
      "Butterscotch Shake\n",
      "Chocolate Flattery\n",
      "Bubblegum \n",
      "Chitchat Kit-Kat \n",
      "Oreo Drama \n",
      "Lover's Dream \n",
      "(Red Velvet)\n",
      "Brownie Shake \n",
      "Nutella Fever \n",
      "Lotus Biscoff Shake\n",
      "Kesar & Rasmalai\n",
      "Ferrero Rocher\n",
      "Cheeku \n",
      "Tender Coconut \n",
      "Sitafal  \n",
      "Blue-Berry \n",
      "Mixed Berries \n",
      "Straw-Bae-Rry\n",
      "COTD \n",
      "(Cold Coffee Of The Day) \n",
      "Cappuccino \n",
      "Mocha Caramel Frappe\n",
      "Irish Cream Latte\n",
      "Tiramisu Latte\n",
      "₹ 79\n",
      "₹ 95\n",
      "₹ 115\n",
      "₹120\n",
      "₹ 149\n",
      "₹ 105\n",
      "₹ 105\n",
      "₹ 90\n",
      "₹ 105\n",
      "₹ 120\n",
      "₹160\n",
      "₹ 99\n",
      "₹ 115\n",
      "₹ 110\n",
      "₹ 139\n",
      "₹ 129\n",
      "₹ 129\n",
      "₹ 139\n",
      "₹ 149\n",
      "₹ 210\n",
      "₹ 149\n",
      "₹ 189\n",
      "₹ 105\n",
      "₹ 105\n",
      "₹ 129\n",
      "₹ 149\n",
      "₹ 159\n",
      "₹ 149\n",
      "₹ 99\n",
      "₹ 120\n",
      "₹ 159\n",
      "₹ 180\n",
      "₹150\n",
      "S\n",
      "M\n",
      "L\n",
      "₹ 215\n",
      "₹ 215\n",
      "₹ 225\n",
      "₹ 230\n",
      "₹ 299\n",
      "₹ 279\n",
      "₹ 279\n",
      "₹ 305\n",
      "₹ 325\n",
      "₹ 325\n",
      "₹ 255\n",
      "₹ 255\n",
      "₹ 265\n",
      "₹ 269\n",
      "₹ 329\n",
      "₹ 329\n",
      "₹ 319\n",
      "₹ 340\n",
      "₹ 365\n",
      "₹ 350\n",
      "{'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-05-07T10:50:50+00:00', 'source': '../data/sample2.pdf', 'file_path': '../data/sample2.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': 'MK Food Menu Landscape', 'author': 'Sejal Ratra', 'subject': '', 'keywords': 'DAGI8D-P_aw,BADLN80vuDY,0', 'moddate': '2025-05-07T10:50:49+00:00', 'trapped': '', 'modDate': \"D:20250507105049+00'00'\", 'creationDate': \"D:20250507105050+00'00'\", 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(file_path=\"../data/sample2.pdf\")\n",
    "docss=loader.load()\n",
    "\n",
    "print(len(docss))\n",
    "print(docss[0].page_content)\n",
    "print(docss[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46c154b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "The Audiovisual \n",
      "In March of 1995, a limousine carrying Ted Koppel, the host of ABC-TV's “Nightline” pulled up to the \n",
      "snow-covered curb outside Morrie's house in West Newton, Massachusetts. \n",
      "Morrie was in a wheelchair full-time now, getting used to helpers lifting him like a heavy sack from the \n",
      "chair to the bed and the bed to the chair. He had begun to cough while eating, and chewing was a chore. \n",
      "His legs were dead; he would never walk again. \n",
      " \n",
      "Yet he refused to be depressed. Instead, Morrie had become a lightning rod of ideas. He jotted down his \n",
      "thoughts on yellow pads, envelopes, folders, scrap paper. He wrote bite-sized philosophies about living \n",
      "with death's shadow: “Accept what you are able to do and what \n",
      "you are not able to do”; “Accept the past as past, without denying it or discarding it”; “Learn to forgive \n",
      "yourself and to forgive others”; “Don't assume that it's too late to get involved.” \n",
      "After a while, he had more than fifty of these “aphorisms,” which he shared with his friends. One friend, \n",
      "a fellow Brandeis professor named Maurie Stein, was so taken with the words that he sent them to a \n",
      "Boston Globe reporter, who came out and wrote a long feature story on Morrie. The headline read:\n",
      "{'producer': 'GPL Ghostscript 8.15', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': 'D:20080607104704', 'source': '../data/sample3.pdf', 'file_path': '../data/sample3.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'Microsoft Word - tuedays with morrie', 'author': 'dixon', 'subject': '', 'keywords': '', 'moddate': 'D:20080607104704', 'trapped': '', 'modDate': 'D:20080607104704', 'creationDate': 'D:20080607104704', 'page': 10}\n"
     ]
    }
   ],
   "source": [
    "###MAIN PDF WE ARE USING GOING AHEAD \n",
    "\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(file_path=\"../data/sample3.pdf\")\n",
    "docsnew=loader.load()\n",
    "\n",
    "print(len(docsnew))\n",
    "print(docsnew[10].page_content)\n",
    "print(docsnew[10].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c7f1f",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7d2f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TExt splitting get into chunks \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "def split_documents(docsnew,chunk_size=600,chunk_overlap=100):\n",
    "    text_splitter= RecursiveCharacterTextSplitter(\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        chunk_size=chunk_size,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(docsnew)\n",
    "    print(f\"Split{len(docsnew)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    if split_docs:\n",
    "        print(f\"\\n Example chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}....\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "290b0736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split88 documents into 413 chunks\n",
      "\n",
      " Example chunk:\n",
      "Content: TUESDAYS  \n",
      "WITH  \n",
      "MORRIE: \n",
      " \n",
      "AN OLD MAN,  \n",
      "A YOUNG MAN,  \n",
      "AND  \n",
      "LIFE'S GREATEST LESSON \n",
      " \n",
      " \n",
      " \n",
      " Mitch Albom....\n",
      "Metadata: {'producer': 'GPL Ghostscript 8.15', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': 'D:20080607104704', 'source': '../data/sample3.pdf', 'file_path': '../data/sample3.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'Microsoft Word - tuedays with morrie', 'author': 'dixon', 'subject': '', 'keywords': '', 'moddate': 'D:20080607104704', 'trapped': '', 'modDate': 'D:20080607104704', 'creationDate': 'D:20080607104704', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(docsnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58930bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split85 documents into 411 chunks\n",
      "\n",
      " Example chunk:\n",
      "Content: Acknowledgments \n",
      "I would like to acknowledge the enormous help given to me in creating this book. For their memories, \n",
      "their patience, and their guidance, I wish to thank Charlotte, Rob, and Jonathan ....\n",
      "Metadata: {'producer': 'GPL Ghostscript 8.15', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': 'D:20080607104704', 'source': '../data/sample3.pdf', 'file_path': '../data/sample3.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'Microsoft Word - tuedays with morrie', 'author': 'dixon', 'subject': '', 'keywords': '', 'moddate': 'D:20080607104704', 'trapped': '', 'modDate': 'D:20080607104704', 'creationDate': 'D:20080607104704', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "# CLEANING PART OF CHUNKING REMOVING UNNECESARRY PAGES LIKE TITLE PAGES AND OTHER CREDITS BASED ON PAGE OR DOCUMENT LENGTH \n",
    "\n",
    "clean_docs = [\n",
    "    d for d in docsnew\n",
    "    if len(d.page_content.strip()) > 300\n",
    "]\n",
    "\n",
    "\n",
    "new_chunks = split_documents(clean_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dc3185",
   "metadata": {},
   "source": [
    "# extra info \n",
    "\n",
    "we can add manual docs to this as well for our convinience as inour case we took the book \"Tuesdays with morries\" \n",
    "and we decided to leave out pages like index and title, any \"docs\" which had less than 300 words \n",
    "which resulted is us excluding the author of the book from our database as it was in the Title page and it did not fit in our constraints of the pages we have kept in our clean_docs \n",
    "which will or may result in model hallucinations if asked bout the author \n",
    "in that case we can make out manual doc like:\n",
    "\n",
    "\n",
    " page_content = \"The book 'Tuesdays with Morrie' is written by Mitch Albom.\"\n",
    "metadata = {\"type\": \"document_info\"}\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637fc231",
   "metadata": {},
   "source": [
    "# Embeddings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (5.2.1)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from sentence-transformers) (5.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from sentence-transformers) (1.3.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from sentence-transformers) (2.10.0)\n",
      "Requirement already satisfied: numpy in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: scipy in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from sentence-transformers) (1.17.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: shellingham in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.10.2)\n",
      "Requirement already satisfied: colorama in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: anyio in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\minor projects\\minor2\\.venv\\lib\\site-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (8.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#### %pip install sentence-transformers ###########\n",
    "\n",
    "#since the venv environment wasn't active i was forced to install it again this way \n",
    "#hence always activate venv first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7113b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156badb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,Dict,Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28c88fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model= SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# texts= [c.page_content for c in chunks]\n",
    "\n",
    "# embeddings = model.encode(texts, show_progress_bar=True)\n",
    "# # # what is encode doing here:\n",
    "# # \n",
    "# # #  for each chunk text:\n",
    "# #     run it through the transformer\n",
    "# #     compress its meaning into 384 numbers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(embeddings.shape)\n",
    "\n",
    "# #  [number_of_chunks , embedding_dimension]\n",
    "# #   → (413 , 384)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b32c96",
   "metadata": {},
   "source": [
    "\n",
    "413 = how many chunks you fed in\n",
    "\n",
    "384 = how many semantic features the model uses\n",
    "\n",
    "## EXAMPLE:\n",
    "\n",
    "### STEP 1: EMBEDDING OUTPUT SHAPE\n",
    "\n",
    "Assume we have 3 chunks:\n",
    "\n",
    "C1: \"Morrie teaches lessons about life and death.\"\n",
    "\n",
    "C2: \"The book discusses love, forgiveness, and acceptance.\"\n",
    "\n",
    "C3: \"A recipe for making pasta with cheese.\"\n",
    "\n",
    "\n",
    "these are the 3 chunks which when inputed the embeddings gives us 7 semantic features for all the chunks \n",
    "\n",
    "### STEP 2: WHAT THOSE 7 FEATURES ARE\n",
    "\n",
    "Let’s pretend the 7 dimensions loosely correspond to:\n",
    "\n",
    "1. life / philosophy\n",
    "\n",
    "2. emotions / relationships\n",
    "\n",
    "3. death / mortality\n",
    "\n",
    "4. food / cooking\n",
    "\n",
    "5. actions / processes\n",
    "\n",
    "6. positivity\n",
    "\n",
    "7. negativity\n",
    "\n",
    "### STEP 3: EXAMPLE EMBEDDINGS (FAKE NUMBERS, REAL IDEA)\n",
    "\n",
    "C1 → [0.9, 0.6, 0.8, 0.0, 0.2, 0.5, 0.1]\n",
    "\n",
    "C2 → [0.8, 0.9, 0.3, 0.0, 0.1, 0.7, 0.0]\n",
    "\n",
    "C3 → [0.0, 0.1, 0.0, 0.9, 0.6, 0.2, 0.1]\n",
    "\n",
    "\n",
    "for all the three chunks the higher number represents the similarity of that chunk to that feature,\n",
    "\n",
    "so for C1 the highest is 0.9 which is the 1st feature (i.e life/philosphy) which is true the most similar feature for C1 indeed \n",
    "\n",
    "is the 1st feature and some may say the 3rd which has 0.8, but we get the idea \n",
    "\n",
    "### STEP 4: WHY SIMILARITY SEARCH WORKS \n",
    "\n",
    "if you ask:\n",
    "\n",
    "\"What lessons does Morrie teach about life?\"\n",
    "\n",
    "That question becomes another 7-dim vector, say:\n",
    "\n",
    "Q → [0.85, 0.7, 0.6, 0.0, 0.2, 0.6, 0.1]\n",
    "\n",
    "now since the this question is embedded into vector and the 1st feature has 0.85 magnitude \n",
    "\n",
    "it is closest to C1 and C2 distance wise and C3 not \n",
    "\n",
    "and if we look at the question logically from our pov it makes sense to retrieve C1 and C2 to answer the question \n",
    "\n",
    "cause they are closely related and almost answers the whole question, now add LLM to this and it will be nicely rephrased \n",
    "\n",
    "and the answer will be presented to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a507455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model:all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MINOR PROJECTS\\MINOR2\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\KUSHAGRA\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1016.44it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully, Embedding dimension:384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x23daf4e55e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformers\"\"\"\n",
    "    def __init__(self,model_name:str =\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Intialize the embeddings manager\n",
    "\n",
    "        args:\n",
    "        model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model() ### we use \"_\" underscores because it is how a protected function is written in python.\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model:{self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully, Embedding dimension:{self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error Loading model:{self.model_name}: {e}\")\n",
    "            raise\n",
    "    def generate_embeddings(self, text: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(text)} texts...\")\n",
    "        embeddings= self.model.encode(text, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape:{embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "\n",
    "### we can intialize the embedding manager \n",
    "\n",
    "embedding_manager= EmbeddingManager()\n",
    "embedding_manager\n",
    "\n",
    "\n",
    "# Embedding size is fixed inside the model architecture, not decided at runtime.\n",
    "\n",
    "# all-MiniLM-L6-v2 was trained to output vectors of length 384.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfd8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
