{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8819f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Document Data-Structure\n",
    "\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c26a8f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'xyz', 'pages': 1, 'author': 'kushagra', 'date_created': '2026-01-01'}, page_content='this is the main text to create a RAG')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc= Document(\n",
    "    page_content= \"this is the main text to create a RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"xyz\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"kushagra\",\n",
    "        \"date_created\":\"2026-01-01\"\n",
    "\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2906e521",
   "metadata": {},
   "source": [
    "## MANUAL LOADER\n",
    "\n",
    "flattened structured fields into a single dense line with | separators.\n",
    "\n",
    "That kills semantic boundaries. Embeddings don’t understand “fields”, they understand language structure.\n",
    "\n",
    "\"Index:1|Name:Thermostat Drone Heater|Description:Consumer approach...\" is embedding garbage\n",
    "\n",
    "# Use a manual loader only if:\n",
    "\n",
    "the official loader cannot extract text correctly at all, or\n",
    "\n",
    "you need non-standard parsing (weird formats, mixed encodings, broken structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "244f9af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,\n",
       " Document(metadata={'source': 'sample.csv', 'row': 0}, page_content='Index:1|Name:Thermostat Drone Heater|Description:Consumer approach woman us those star.|Brand:Bradford-Yu|Category:Kitchen Appliances|Price:74|Currency:USD|Stock:139|EAN:8619793560985|Color:Orchid|Size:Medium|Availability:backorder|Internal ID:38'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/sample.csv\")\n",
    "documents=[]\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = \"|\".join(f\"{col}:{row[col]}\" for col in df.columns )\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content= text,\n",
    "        metadata={\n",
    "            \"source\": \"sample.csv\",\n",
    "            \"row\":idx\n",
    "        }\n",
    "    )\n",
    "\n",
    "    documents.append(doc)\n",
    "len(documents), documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7303e1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'sample.csv', 'row': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8bec3f",
   "metadata": {},
   "source": [
    "# LANGCHAIN LOADER\n",
    "\n",
    "Each field is clearly separated.\n",
    "\n",
    "Newlines preserve semantic breaks.\n",
    "\n",
    "Metadata is clean and minimal.\n",
    "\n",
    "This will chunk better, retrieve better, and hallucinate less.\n",
    "\n",
    "\n",
    "that said it is not that manual loader is bad it can come to good when used with a better script and then you can manipulate the metadata tags to keep much more relevant info going forward.\n",
    "\n",
    "# extra\n",
    "\n",
    "If it’s useful for meaning, it belongs in page_content.\n",
    "\n",
    "If it’s useful for control or tracing, it belongs in metadata.\n",
    "\n",
    "\n",
    "A better loader does two independent things well:\n",
    "\n",
    "1.produces clean, human-readable page-content.\n",
    "\n",
    "2.produces minimal, accurate metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "513531a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Index: 1\n",
      "Name: Thermostat Drone Heater\n",
      "Description: Consumer approach woman us those star.\n",
      "Brand: Bradford-Yu\n",
      "Category: Kitchen Appliances\n",
      "Price: 74\n",
      "Currency: USD\n",
      "Stock: 139\n",
      "EAN: 8619793560985\n",
      "Color: Orchid\n",
      "Size: Medium\n",
      "Availability: backorder\n",
      "Internal ID: 38\n",
      "{'source': '../data/sample.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path=\"../data/sample.csv\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ff5cb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Mumma's Kitchen\n",
      "Twister\n",
      "Sandwich  &\n",
      "Hot Dog\n",
      "Cheese Chilli \n",
      "Peri Peri \n",
      "Schezwan \n",
      "Lemon Chilli \n",
      "Italian \n",
      "Tandoori Masala\n",
      "Cheese Chutney\n",
      "Cheese Chilli \n",
      "Chatpata Indori\n",
      "Cheese Corn \n",
      "Paneer Schezwan\n",
      "Cheese Burst\n",
      "Chatpata Hot Dog \n",
      "Veg Aalu Tikki Hot Dog \n",
      "Paneer Tikka Hot Dog \n",
      "₹ 85\n",
      "₹ 95\n",
      "₹ 115\n",
      "₹ 105\n",
      "₹ 125\n",
      "₹ 135\n",
      "₹ 129\n",
      "₹ 135\n",
      "₹ 129\n",
      "₹ 89\n",
      "₹ 99\n",
      "₹ 99\n",
      "₹ 99\n",
      "₹ 89\n",
      "₹ 89\n",
      "Pizzas\n",
      "Pasta\n",
      "Go-To \n",
      "Indie-Mexican \n",
      "Oh-Cheese! \n",
      "Desi Chirpira \n",
      "Toofani Mexican\n",
      "Crunchy Kurkure \n",
      "Peri-Peri Spicy\n",
      "Pro-Max Cheese \n",
      "Paneer Shaukeen\n",
      "Cheesy Fries Supreme\n",
      "Sab Par Bhari \n",
      "Pasta Arrabiata \n",
      "(Penne pasta tossed in authentic\n",
      "red sauce)\n",
      "Pasta Alfredo \n",
      "(Penne pasta tossed in creamy\n",
      "white sauce)\n",
      "Baked Cheesy Pasta\n",
      "(Arrabiata pasta baked to\n",
      "perfection with extra cheese)\n",
      "Baked Alfredo \n",
      "Green Wave \n",
      "(Capsicum, Jalapeno, Onion) \n",
      "Farm Fresh \n",
      "(The evergreen combination of Onion and Capsicum) \n",
      "Margherita \n",
      "(The classic pizza sauce and mozzarella cheese) \n",
      "Corn Feast \n",
      "(Golden Corn and lots of cheese) \n",
      "Veggie Blast \n",
      "(Capsicum, Onion, Golden Corn, Olive)\n",
      "Smoky Hot \n",
      "(A spice lover's heaven with Red Paprika, Jalapeno and Onion) \n",
      "Mushroom Riot \n",
      "(Smoky Grilled Mushrooms with Onion, Capsicum and Olives) \n",
      "3 Sins of Cheese \n",
      "(Three types of cheese with cheese burst crust) \n",
      "Paneer Patola \n",
      "(Indian Twist of Tandoori Paneer, Onion, Capsicum, Red\n",
      "Paprika) \n",
      "Grandiose \n",
      "(Onion, Capsicum, Olives, red paprika, Golden Corn, Jalapeno)\n",
      "₹ 125\n",
      "₹ 125\n",
      "₹ 135\n",
      "₹ 140\n",
      "₹ 199\n",
      "₹ 199\n",
      "₹ 189\n",
      "₹ 210\n",
      "₹ 235\n",
      "₹ 230\n",
      "₹ 79\n",
      "₹ 89\n",
      "₹ 99\n",
      "₹ 105\n",
      "₹ 125\n",
      "₹ 99\n",
      "₹ 105\n",
      "₹ 135\n",
      "₹ 129\n",
      "₹ 139\n",
      "₹ 149\n",
      "₹ 169\n",
      "₹ 195\n",
      "₹ 215\n",
      "₹235\n",
      "Patties/Puff\n",
      "Garlic Breads\n",
      "Masala Patties\n",
      "Cheese patties\n",
      "Cheese-Corn\n",
      "patties\n",
      "Pizza Patties \n",
      "Paneer Barbeque\n",
      "Patties\n",
      "Cheese Corn \n",
      "Paprika & Olives\n",
      "Jalapeno & Onion\n",
      "Cheesy Bread Sticks\n",
      "Stuffed Cheese Corn\n",
      "Stuffed tandoori\n",
      "Paneer\n",
      "₹ 120\n",
      "₹ 135\n",
      "₹ 135\n",
      "₹ 145\n",
      "₹ 185\n",
      "₹ 199\n",
      "₹ 35\n",
      "₹ 55\n",
      " ₹ 70\n",
      "₹ 65\n",
      "₹ 75\n",
      "Burgers\n",
      "Sides\n",
      "Shakes &\n",
      "Coffee\n",
      "French Fries - Salted\n",
      "Peri Peri French Fries\n",
      "Cheesy French Fries\n",
      "Cheesy Peri Peri Fries\n",
      "Super Loaded Fries\n",
      "Kulhad Maggi \n",
      "Korean Cheese Bun \n",
      "Veg Fried Momos\n",
      "Paneer Fried Momos\n",
      "Fried Cheesy Momos\n",
      "Mexican Nacho Bowl\n",
      "Butterscotch Shake\n",
      "Chocolate Flattery\n",
      "Bubblegum \n",
      "Chitchat Kit-Kat \n",
      "Oreo Drama \n",
      "Lover's Dream \n",
      "(Red Velvet)\n",
      "Brownie Shake \n",
      "Nutella Fever \n",
      "Lotus Biscoff Shake\n",
      "Kesar & Rasmalai\n",
      "Ferrero Rocher\n",
      "Cheeku \n",
      "Tender Coconut \n",
      "Sitafal  \n",
      "Blue-Berry \n",
      "Mixed Berries \n",
      "Straw-Bae-Rry\n",
      "COTD \n",
      "(Cold Coffee Of The Day) \n",
      "Cappuccino \n",
      "Mocha Caramel Frappe\n",
      "Irish Cream Latte\n",
      "Tiramisu Latte\n",
      "₹ 79\n",
      "₹ 95\n",
      "₹ 115\n",
      "₹120\n",
      "₹ 149\n",
      "₹ 105\n",
      "₹ 105\n",
      "₹ 90\n",
      "₹ 105\n",
      "₹ 120\n",
      "₹160\n",
      "₹ 99\n",
      "₹ 115\n",
      "₹ 110\n",
      "₹ 139\n",
      "₹ 129\n",
      "₹ 129\n",
      "₹ 139\n",
      "₹ 149\n",
      "₹ 210\n",
      "₹ 149\n",
      "₹ 189\n",
      "₹ 105\n",
      "₹ 105\n",
      "₹ 129\n",
      "₹ 149\n",
      "₹ 159\n",
      "₹ 149\n",
      "₹ 99\n",
      "₹ 120\n",
      "₹ 159\n",
      "₹ 180\n",
      "₹150\n",
      "S\n",
      "M\n",
      "L\n",
      "₹ 215\n",
      "₹ 215\n",
      "₹ 225\n",
      "₹ 230\n",
      "₹ 299\n",
      "₹ 279\n",
      "₹ 279\n",
      "₹ 305\n",
      "₹ 325\n",
      "₹ 325\n",
      "₹ 255\n",
      "₹ 255\n",
      "₹ 265\n",
      "₹ 269\n",
      "₹ 329\n",
      "₹ 329\n",
      "₹ 319\n",
      "₹ 340\n",
      "₹ 365\n",
      "₹ 350\n",
      "{'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-05-07T10:50:50+00:00', 'source': '../data/sample2.pdf', 'file_path': '../data/sample2.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': 'MK Food Menu Landscape', 'author': 'Sejal Ratra', 'subject': '', 'keywords': 'DAGI8D-P_aw,BADLN80vuDY,0', 'moddate': '2025-05-07T10:50:49+00:00', 'trapped': '', 'modDate': \"D:20250507105049+00'00'\", 'creationDate': \"D:20250507105050+00'00'\", 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(file_path=\"../data/sample2.pdf\")\n",
    "docss=loader.load()\n",
    "\n",
    "print(len(docss))\n",
    "print(docss[0].page_content)\n",
    "print(docss[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c154b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "The Audiovisual \n",
      "In March of 1995, a limousine carrying Ted Koppel, the host of ABC-TV's “Nightline” pulled up to the \n",
      "snow-covered curb outside Morrie's house in West Newton, Massachusetts. \n",
      "Morrie was in a wheelchair full-time now, getting used to helpers lifting him like a heavy sack from the \n",
      "chair to the bed and the bed to the chair. He had begun to cough while eating, and chewing was a chore. \n",
      "His legs were dead; he would never walk again. \n",
      " \n",
      "Yet he refused to be depressed. Instead, Morrie had become a lightning rod of ideas. He jotted down his \n",
      "thoughts on yellow pads, envelopes, folders, scrap paper. He wrote bite-sized philosophies about living \n",
      "with death's shadow: “Accept what you are able to do and what \n",
      "you are not able to do”; “Accept the past as past, without denying it or discarding it”; “Learn to forgive \n",
      "yourself and to forgive others”; “Don't assume that it's too late to get involved.” \n",
      "After a while, he had more than fifty of these “aphorisms,” which he shared with his friends. One friend, \n",
      "a fellow Brandeis professor named Maurie Stein, was so taken with the words that he sent them to a \n",
      "Boston Globe reporter, who came out and wrote a long feature story on Morrie. The headline read:\n",
      "{'producer': 'GPL Ghostscript 8.15', 'creator': 'PScript5.dll Version 5.2.2', 'creationdate': 'D:20080607104704', 'source': '../data/sample3.pdf', 'file_path': '../data/sample3.pdf', 'total_pages': 88, 'format': 'PDF 1.4', 'title': 'Microsoft Word - tuedays with morrie', 'author': 'dixon', 'subject': '', 'keywords': '', 'moddate': 'D:20080607104704', 'trapped': '', 'modDate': 'D:20080607104704', 'creationDate': 'D:20080607104704', 'page': 10}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(file_path=\"../data/sample3.pdf\")\n",
    "docsnew=loader.load()\n",
    "\n",
    "print(len(docsnew))\n",
    "print(docsnew[10].page_content)\n",
    "print(docsnew[10].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d2f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TExt splitting get into chunks \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "def split_documents(docsnew,chunk_size=600,chunk_overlap=100):\n",
    "    text_splitter= RecursiveCharacterTextSplitter(\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        chunk_size=chunk_size,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(docsnew)\n",
    "    print(f\"Split{len(docsnew)} documents into {len(split_docs)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b0736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
